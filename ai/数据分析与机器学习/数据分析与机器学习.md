# 数据分析与机器学习

## 06 线性回归原理推导

概念：找到最合适的一条线（或者一个面）来更好的拟合我们的数据点

### **误差**

真实值和预测值之间存在的差异。
$$
\epsilon
$$
误差是独立并且具有相同的分布，并且服务均值为0方差为
$$
\theta^2
$$
的高斯分布。

独立：两个误差之间没有任何关系，一个误差的值不会影响另一个。

高斯分布：绝大部分情况下，误差不会太大，极小情况下浮动会比较大。

误差服从高斯分布是一个假设，这个是机器学习的基础。

**似然函数**：根据样本去估计参数值，用数据去推服从的规则，使得在这些参数的情况下，样本值和真实值的误差越小越好。

**极大似然估计**：让似然函数越大越好
$$
预测值与误差：y^{(i)} = \theta^Tx^{(i)} + \epsilon^{(i)}
$$

$$
由于误差服务高斯分布：p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})
$$

$$
将上两式合并：p(y^{(i)}|x^{i}; \theta) =  \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)} - \theta^Tx{(i)})^2}{2\sigma^2})
$$

![](http://static.zybuluo.com/vermouth9/yz6wk4gfnj68a51heyq4uyhi/%E6%8D%95%E8%8E%B7.PNG)

![](http://static.zybuluo.com/vermouth9/5o9h831st9fwrfefcyiwc1qa/image.png)
$$
也就是让J(\theta)越小越好
$$
